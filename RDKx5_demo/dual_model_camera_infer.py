#!/usr/bin/env python3
"""
åŒæ¨¡å‹æ‘„åƒå¤´å®æ—¶æ£€æµ‹ - å¤šçº¿ç¨‹ç‰ˆæœ¬
åŒæ—¶åŠ è½½ä¸¤ä¸ªbinæ¨¡å‹ï¼Œå¯¹æ‘„åƒå¤´ç”»é¢è¿›è¡Œæ¨ç†ï¼Œç»“æœå åŠ æ˜¾ç¤º
"""
import cv2
import numpy as np
import time
import threading
from queue import Queue
from hobot_dnn import pyeasy_dnn as dnn


class YOLOv11Detector:
    """YOLOv11æ£€æµ‹å™¨ç±»"""
    
    def __init__(self, model_path, conf_thresh=0.3, nms_thresh=0.5, cls_num=80, 
                 class_names=None, color_offset=0, name="Model"):
        """
        åˆå§‹åŒ–æ£€æµ‹å™¨
        
        Args:
            model_path: binæ¨¡å‹è·¯å¾„
            conf_thresh: ç½®ä¿¡åº¦é˜ˆå€¼ (0.0-1.0)
            nms_thresh: NMSé˜ˆå€¼ (0.0-1.0)
            cls_num: ç±»åˆ«æ•°é‡
            class_names: ç±»åˆ«åç§°åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            color_offset: é¢œè‰²åç§»é‡ï¼Œç”¨äºåŒºåˆ†ä¸åŒæ¨¡å‹çš„æ£€æµ‹ç»“æœ
            name: æ¨¡å‹åç§°ï¼Œç”¨äºæ—¥å¿—æ˜¾ç¤º
        """
        self.conf_thresh = conf_thresh
        self.nms_thresh = nms_thresh
        self.input_size = 640
        self.reg_max = 16  # DFLçš„æœ€å¤§å›å½’è·ç¦»
        self.strides = [8, 16, 32]  # ä¸‰ä¸ªæ£€æµ‹å¤´çš„stride
        self.cls_num = cls_num
        self.name = name
        self.color_offset = color_offset
        
        # åŠ è½½æ¨¡å‹
        models = dnn.load(model_path)
        self.model = models[0]
        print(f"âœ… [{self.name}] æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
        
        # é¢„è®¡ç®—anchor gridï¼ˆåŠ é€Ÿåå¤„ç†ï¼‰
        self._init_anchors()
        
        # ç±»åˆ«åç§°
        if class_names is not None:
            self.class_names = class_names
        else:
            # é»˜è®¤COCO 80ç±»ç±»åˆ«åç§°
            self.class_names = [
                'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck',
                'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench',
                'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra',
                'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',
                'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',
                'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',
                'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',
                'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',
                'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse',
                'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',
                'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',
                'toothbrush'
            ]
        
        # ä¸ºæ¯ä¸ªç±»åˆ«ç”Ÿæˆéšæœºé¢œè‰²ï¼ˆå¸¦åç§»ä»¥åŒºåˆ†ä¸åŒæ¨¡å‹ï¼‰
        np.random.seed(42 + color_offset)
        self.colors = np.random.randint(0, 255, size=(max(len(self.class_names), cls_num), 3), dtype=int)
    
    def _init_anchors(self):
        """
        é¢„è®¡ç®—anchor grid
        å¯¹äº640x640è¾“å…¥ï¼Œä¸‰ä¸ªæ£€æµ‹å¤´çš„gridå¤§å°ä¸ºï¼š
        - stride=8:  80x80
        - stride=16: 40x40
        - stride=32: 20x20
        """
        self.grids = []
        for stride in self.strides:
            h = w = self.input_size // stride
            # ç”Ÿæˆç½‘æ ¼åæ ‡ (h, w, 2)
            grid_y, grid_x = np.meshgrid(np.arange(h), np.arange(w), indexing='ij')
            grid = np.stack([grid_x, grid_y], axis=-1).reshape(-1, 2)
            self.grids.append(grid)
    
    def bgr_to_nv12(self, img):
        """
        BGRå›¾ç‰‡è½¬NV12æ ¼å¼ + Letterboxç¼©æ”¾
        
        NV12æ ¼å¼è¯´æ˜ï¼š
        - Yå¹³é¢: 640x640 (äº®åº¦)
        - UVå¹³é¢: 320x640 (è‰²åº¦ï¼ŒUå’ŒVäº¤é”™å­˜å‚¨)
        - æ€»å¤§å°: 640x960
        
        Args:
            img: BGRå›¾ç‰‡ (H, W, 3)
        
        Returns:
            nv12: NV12æ•°æ® (960, 640)
            scale: ç¼©æ”¾æ¯”ä¾‹
            pad_left: å·¦è¾¹padding
            pad_top: ä¸Šè¾¹padding
        """
        h, w = img.shape[:2]
        
        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ï¼ˆä¿æŒå®½é«˜æ¯”ï¼‰
        scale = min(self.input_size / h, self.input_size / w)
        new_h, new_w = int(h * scale), int(w * scale)
        
        # Letterbox resize
        resized = cv2.resize(img, (new_w, new_h))
        canvas = np.full((self.input_size, self.input_size, 3), 114, dtype=np.uint8)
        top = (self.input_size - new_h) // 2
        left = (self.input_size - new_w) // 2
        canvas[top:top+new_h, left:left+new_w] = resized
        
        # BGR to YUV (I420æ ¼å¼)
        yuv = cv2.cvtColor(canvas, cv2.COLOR_BGR2YUV_I420)
        
        # æå–Yã€Uã€Vå¹³é¢
        y = yuv[:self.input_size, :]
        u = yuv[self.input_size:self.input_size+self.input_size//4, :].reshape(
            self.input_size//2, self.input_size//2)
        v = yuv[self.input_size+self.input_size//4:, :].reshape(
            self.input_size//2, self.input_size//2)
        
        # ç»„è£…NV12 (UVäº¤é”™å­˜å‚¨)
        uv = np.empty((self.input_size//2, self.input_size), dtype=np.uint8)
        uv[:, 0::2] = u
        uv[:, 1::2] = v
        
        nv12 = np.concatenate([y, uv], axis=0)
        
        return nv12, scale, left, top
    
    def dfl_decode(self, bbox_raw):
        """
        DFL (Distribution Focal Loss) è§£ç 
        
        å°†64ç»´çš„åˆ†å¸ƒç‰¹å¾è§£ç ä¸º4ç»´çš„bboxåæ ‡(ltrb)
        
        åŸç†ï¼š
        1. å°†64ç»´reshapeä¸º(4, 16)ï¼Œæ¯ä¸ªæ–¹å‘16ä¸ªbin
        2. å¯¹æ¯ä¸ªæ–¹å‘åšSoftmaxï¼Œå¾—åˆ°æ¦‚ç‡åˆ†å¸ƒ
        3. è®¡ç®—æœŸæœ›å€¼ï¼ˆåŠ æƒæ±‚å’Œï¼‰ä½œä¸ºæœ€ç»ˆè·ç¦»
        
        Args:
            bbox_raw: (N, 64) DFLç‰¹å¾
        
        Returns:
            ltrb: (N, 4) è¾¹ç•Œæ¡†è·ç¦»(left, top, right, bottom)
        """
        # Reshape: (N, 64) -> (N, 4, 16)
        bbox = bbox_raw.reshape(-1, 4, self.reg_max)
        
        # Softmaxå½’ä¸€åŒ–
        bbox_exp = np.exp(bbox - np.max(bbox, axis=-1, keepdims=True))
        bbox_softmax = bbox_exp / np.sum(bbox_exp, axis=-1, keepdims=True)
        
        # è®¡ç®—æœŸæœ›å€¼ (åŠ æƒæ±‚å’Œ)
        weights = np.arange(self.reg_max).reshape(1, 1, -1)
        ltrb = np.sum(bbox_softmax * weights, axis=-1)
        
        return ltrb
    
    def detect(self, img, nv12_data=None, preprocess_info=None):
        """
        æ‰§è¡Œç›®æ ‡æ£€æµ‹
        
        æµç¨‹ï¼š
        1. é¢„å¤„ç†ï¼šBGR -> NV12ï¼ˆå¦‚æœæœªæä¾›ï¼‰
        2. BPUæ¨ç†ï¼šforward
        3. åå¤„ç†ï¼šè§£ç  + NMS
        
        Args:
            img: è¾“å…¥å›¾ç‰‡ (BGRæ ¼å¼)
            nv12_data: é¢„å¤„ç†åçš„NV12æ•°æ®ï¼ˆå¯é€‰ï¼Œç”¨äºå…±äº«é¢„å¤„ç†ç»“æœï¼‰
            preprocess_info: é¢„å¤„ç†ä¿¡æ¯ (scale, pad_left, pad_top)ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            boxes: æ£€æµ‹æ¡† (N, 4) xyxyæ ¼å¼
            scores: ç½®ä¿¡åº¦ (N,)
            classes: ç±»åˆ«ID (N,)
        """
        orig_h, orig_w = img.shape[:2]
        
        # 1. é¢„å¤„ç†ï¼ˆå¦‚æœæœªæä¾›é¢„å¤„ç†æ•°æ®ï¼‰
        if nv12_data is None:
            nv12, scale, pad_left, pad_top = self.bgr_to_nv12(img)
        else:
            nv12 = nv12_data
            scale, pad_left, pad_top = preprocess_info
        
        # 2. BPUæ¨ç†
        outputs = self.model.forward(nv12)
        
        # 3. åå¤„ç†
        boxes, scores, classes = self._postprocess(
            outputs, scale, pad_left, pad_top, orig_w, orig_h
        )
        
        return boxes, scores, classes
    
    def _postprocess(self, outputs, scale, pad_left, pad_top, orig_w, orig_h):
        """
        åå¤„ç†ï¼šè§£ç  + ç­›é€‰ + NMS
        
        è¾“å‡ºæ ¼å¼ï¼š
        - outputs[0-2]: bboxç‰¹å¾ (stride=8/16/32)
        - outputs[3-5]: classåˆ†æ•° (stride=8/16/32)
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        - åˆ©ç”¨Sigmoidå•è°ƒæ€§ï¼Œå…ˆç­›é€‰å†è®¡ç®—
        - å‡å°‘ä¸å¿…è¦çš„DFLè§£ç 
        """
        all_boxes = []
        all_scores = []
        all_classes = []
        
        # åˆ†ç¦»bboxå’Œclsè¾“å‡º
        bbox_outputs = outputs[:3]
        cls_outputs = outputs[3:]
        
        # éå†ä¸‰ä¸ªæ£€æµ‹å¤´
        for i, (bbox_out, cls_out, grid, stride) in enumerate(
            zip(bbox_outputs, cls_outputs, self.grids, self.strides)):
            
            # è·å–åŸå§‹è¾“å‡º (é‡åŒ–åçš„int16æ•°æ®ä¼šè‡ªåŠ¨è½¬ä¸ºfloat32)
            bbox_feat = bbox_out.buffer.reshape(-1, 64)   # (H*W, 64)
            cls_feat = cls_out.buffer.reshape(-1, self.cls_num)  # (H*W, cls_num)
            
            # ====== ä¼˜åŒ–ï¼šå…ˆç­›é€‰å†è®¡ç®— ======
            # Sigmoidæ˜¯å•è°ƒå‡½æ•°ï¼Œå¯ä»¥åœ¨logitç©ºé—´ç›´æ¥æ¯”è¾ƒ
            cls_max = np.max(cls_feat, axis=1)
            
            # è®¡ç®—é˜ˆå€¼å¯¹åº”çš„logitå€¼
            # sigmoid(x) > thresh  <==>  x > log(thresh / (1-thresh))
            thresh_logit = np.log(self.conf_thresh / (1 - self.conf_thresh))
            
            # ç­›é€‰é«˜ç½®ä¿¡åº¦å€™é€‰æ¡†
            valid_mask = cls_max > thresh_logit
            
            if not np.any(valid_mask):
                continue
            
            # åªå¯¹æœ‰æ•ˆå€™é€‰æ¡†è¿›è¡Œåç»­è®¡ç®—
            valid_bbox = bbox_feat[valid_mask]
            valid_cls = cls_feat[valid_mask]
            valid_grid = grid[valid_mask]
            
            # ====== ç±»åˆ«åˆ†æ•°è®¡ç®— ======
            # Sigmoidæ¿€æ´»
            scores = 1 / (1 + np.exp(-valid_cls))
            max_scores = np.max(scores, axis=1)
            max_classes = np.argmax(scores, axis=1)
            
            # ====== è¾¹ç•Œæ¡†è§£ç  ======
            # DFLè§£ç å¾—åˆ°ltrbè·ç¦»
            ltrb = self.dfl_decode(valid_bbox)
            
            # è®¡ç®—anchorä¸­å¿ƒç‚¹åæ ‡
            x_center = (valid_grid[:, 0] + 0.5) * stride
            y_center = (valid_grid[:, 1] + 0.5) * stride
            
            # ltrbè½¬xyxyï¼ˆå»é™¤paddingï¼Œè¿˜åŸåˆ°åŸå›¾å°ºåº¦ï¼‰
            x1 = (x_center - ltrb[:, 0] * stride - pad_left) / scale
            y1 = (y_center - ltrb[:, 1] * stride - pad_top) / scale
            x2 = (x_center + ltrb[:, 2] * stride - pad_left) / scale
            y2 = (y_center + ltrb[:, 3] * stride - pad_top) / scale
            
            # è£å‰ªåˆ°å›¾åƒè¾¹ç•Œ
            x1 = np.clip(x1, 0, orig_w)
            y1 = np.clip(y1, 0, orig_h)
            x2 = np.clip(x2, 0, orig_w)
            y2 = np.clip(y2, 0, orig_h)
            
            boxes = np.stack([x1, y1, x2, y2], axis=1)
            
            all_boxes.append(boxes)
            all_scores.append(max_scores)
            all_classes.append(max_classes)
        
        if not all_boxes:
            return np.array([]), np.array([]), np.array([])
        
        # ====== åˆå¹¶æ‰€æœ‰å°ºåº¦çš„æ£€æµ‹ç»“æœ ======
        boxes = np.concatenate(all_boxes, axis=0)
        scores = np.concatenate(all_scores, axis=0)
        classes = np.concatenate(all_classes, axis=0)
        
        # ====== NMSå»é‡ ======
        indices = cv2.dnn.NMSBoxes(
            boxes.tolist(),
            scores.tolist(),
            self.conf_thresh,
            self.nms_thresh
        )
        
        if len(indices) > 0:
            indices = indices.flatten()
            return boxes[indices], scores[indices], classes[indices]
        
        return np.array([]), np.array([]), np.array([])
    
    def draw(self, img, boxes, scores, classes, prefix=""):
        """
        åœ¨å›¾ç‰‡ä¸Šç»˜åˆ¶æ£€æµ‹ç»“æœ
        
        Args:
            img: è¾“å…¥å›¾ç‰‡
            boxes: æ£€æµ‹æ¡†
            scores: ç½®ä¿¡åº¦
            classes: ç±»åˆ«ID
            prefix: æ ‡ç­¾å‰ç¼€ï¼ˆç”¨äºåŒºåˆ†ä¸åŒæ¨¡å‹ï¼‰
        
        Returns:
            img: ç»˜åˆ¶åçš„å›¾ç‰‡
        """
        for box, score, cls in zip(boxes, scores, classes):
            x1, y1, x2, y2 = map(int, box)
            cls_idx = int(cls)
            color = tuple(map(int, self.colors[cls_idx % len(self.colors)]))
            
            # è·å–ç±»åˆ«åç§°
            if cls_idx < len(self.class_names):
                class_name = self.class_names[cls_idx]
            else:
                class_name = f"class_{cls_idx}"
            
            label = f"{prefix}{class_name}: {score:.2f}"
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)
            
            # ç»˜åˆ¶æ ‡ç­¾ï¼ˆå¸¦èƒŒæ™¯ï¼‰
            (label_w, label_h), _ = cv2.getTextSize(
                label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2
            )
            cv2.rectangle(img, (x1, y1-label_h-10), (x1+label_w, y1), color, -1)
            cv2.putText(img, label, (x1, y1-5),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
        
        return img


class InferenceThread(threading.Thread):
    """æ¨ç†çº¿ç¨‹ç±»"""
    
    def __init__(self, detector, input_queue, output_queue, name="InferThread"):
        """
        åˆå§‹åŒ–æ¨ç†çº¿ç¨‹
        
        Args:
            detector: YOLOv11Detectorå®ä¾‹
            input_queue: è¾“å…¥é˜Ÿåˆ—ï¼ˆå¸§æ•°æ®ï¼‰
            output_queue: è¾“å‡ºé˜Ÿåˆ—ï¼ˆæ£€æµ‹ç»“æœï¼‰
            name: çº¿ç¨‹åç§°
        """
        super().__init__(name=name)
        self.detector = detector
        self.input_queue = input_queue
        self.output_queue = output_queue
        self.running = True
        self.daemon = True
    
    def run(self):
        """çº¿ç¨‹ä¸»å¾ªç¯"""
        while self.running:
            try:
                # ä»è¾“å…¥é˜Ÿåˆ—è·å–å¸§æ•°æ®ï¼ˆå¸¦è¶…æ—¶ï¼‰
                data = self.input_queue.get(timeout=0.1)
                if data is None:
                    continue
                
                frame, frame_id, nv12_data, preprocess_info = data
                
                # æ‰§è¡Œæ¨ç†
                boxes, scores, classes = self.detector.detect(
                    frame, nv12_data, preprocess_info
                )
                
                # å°†ç»“æœæ”¾å…¥è¾“å‡ºé˜Ÿåˆ—
                self.output_queue.put((frame_id, boxes, scores, classes))
                
            except Exception as e:
                if self.running:
                    # é˜Ÿåˆ—è¶…æ—¶æ˜¯æ­£å¸¸çš„ï¼Œå…¶ä»–å¼‚å¸¸éœ€è¦è®°å½•
                    if "Empty" not in str(type(e).__name__):
                        print(f"âš ï¸ [{self.name}] æ¨ç†å¼‚å¸¸: {e}")
    
    def stop(self):
        """åœæ­¢çº¿ç¨‹"""
        self.running = False


class DualModelInference:
    """åŒæ¨¡å‹æ¨ç†ç®¡ç†å™¨"""
    
    def __init__(self, model1_config, model2_config):
        """
        åˆå§‹åŒ–åŒæ¨¡å‹æ¨ç†
        
        Args:
            model1_config: æ¨¡å‹1é…ç½®å­—å…¸
            model2_config: æ¨¡å‹2é…ç½®å­—å…¸
        
        é…ç½®å­—å…¸æ ¼å¼ï¼š
        {
            'model_path': str,      # æ¨¡å‹è·¯å¾„
            'conf_thresh': float,   # ç½®ä¿¡åº¦é˜ˆå€¼
            'nms_thresh': float,    # NMSé˜ˆå€¼
            'cls_num': int,         # ç±»åˆ«æ•°é‡
            'class_names': list,    # ç±»åˆ«åç§°ï¼ˆå¯é€‰ï¼‰
            'name': str,            # æ¨¡å‹åç§°
            'label_prefix': str,    # æ ‡ç­¾å‰ç¼€
        }
        """
        print("=" * 70)
        print("ğŸš€ åˆå§‹åŒ–åŒæ¨¡å‹æ¨ç†ç³»ç»Ÿ")
        print("=" * 70)
        
        # åˆ›å»ºæ£€æµ‹å™¨
        self.detector1 = YOLOv11Detector(
            model_path=model1_config['model_path'],
            conf_thresh=model1_config.get('conf_thresh', 0.3),
            nms_thresh=model1_config.get('nms_thresh', 0.5),
            cls_num=model1_config.get('cls_num', 80),
            class_names=model1_config.get('class_names'),
            color_offset=0,
            name=model1_config.get('name', 'Model1')
        )
        
        self.detector2 = YOLOv11Detector(
            model_path=model2_config['model_path'],
            conf_thresh=model2_config.get('conf_thresh', 0.3),
            nms_thresh=model2_config.get('nms_thresh', 0.5),
            cls_num=model2_config.get('cls_num', 80),
            class_names=model2_config.get('class_names'),
            color_offset=100,  # é¢œè‰²åç§»ï¼ŒåŒºåˆ†ä¸¤ä¸ªæ¨¡å‹
            name=model2_config.get('name', 'Model2')
        )
        
        self.label_prefix1 = model1_config.get('label_prefix', '[M1]')
        self.label_prefix2 = model2_config.get('label_prefix', '[M2]')
        
        # åˆ›å»ºé˜Ÿåˆ—
        self.input_queue1 = Queue(maxsize=2)
        self.input_queue2 = Queue(maxsize=2)
        self.output_queue1 = Queue(maxsize=2)
        self.output_queue2 = Queue(maxsize=2)
        
        # åˆ›å»ºæ¨ç†çº¿ç¨‹
        self.thread1 = InferenceThread(
            self.detector1, self.input_queue1, self.output_queue1,
            name=f"Thread-{model1_config.get('name', 'Model1')}"
        )
        self.thread2 = InferenceThread(
            self.detector2, self.input_queue2, self.output_queue2,
            name=f"Thread-{model2_config.get('name', 'Model2')}"
        )
        
        # ç»“æœç¼“å­˜
        self.results_cache = {}
        self.cache_lock = threading.Lock()
        
        print("âœ… åŒæ¨¡å‹æ¨ç†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def start(self):
        """å¯åŠ¨æ¨ç†çº¿ç¨‹"""
        self.thread1.start()
        self.thread2.start()
        print("ğŸƒ æ¨ç†çº¿ç¨‹å·²å¯åŠ¨")
    
    def stop(self):
        """åœæ­¢æ¨ç†çº¿ç¨‹"""
        self.thread1.stop()
        self.thread2.stop()
        self.thread1.join(timeout=1.0)
        self.thread2.join(timeout=1.0)
        print("â¹ï¸ æ¨ç†çº¿ç¨‹å·²åœæ­¢")
    
    def preprocess(self, frame):
        """
        å…±äº«é¢„å¤„ç†
        
        Args:
            frame: è¾“å…¥å¸§
        
        Returns:
            nv12_data: NV12æ•°æ®
            preprocess_info: (scale, pad_left, pad_top)
        """
        nv12, scale, pad_left, pad_top = self.detector1.bgr_to_nv12(frame)
        return nv12, (scale, pad_left, pad_top)
    
    def submit_frame(self, frame, frame_id):
        """
        æäº¤å¸§è¿›è¡Œæ¨ç†
        
        Args:
            frame: è¾“å…¥å¸§
            frame_id: å¸§ID
        """
        # å…±äº«é¢„å¤„ç†ç»“æœ
        nv12_data, preprocess_info = self.preprocess(frame)
        
        # æäº¤åˆ°ä¸¤ä¸ªæ¨ç†é˜Ÿåˆ—
        try:
            self.input_queue1.put_nowait((frame, frame_id, nv12_data, preprocess_info))
        except:
            pass  # é˜Ÿåˆ—æ»¡åˆ™è·³è¿‡
        
        try:
            self.input_queue2.put_nowait((frame, frame_id, nv12_data, preprocess_info))
        except:
            pass  # é˜Ÿåˆ—æ»¡åˆ™è·³è¿‡
    
    def get_results(self, frame_id, timeout=0.05):
        """
        è·å–æ¨ç†ç»“æœ
        
        Args:
            frame_id: å¸§ID
            timeout: è¶…æ—¶æ—¶é—´
        
        Returns:
            result1: (boxes, scores, classes) æˆ– None
            result2: (boxes, scores, classes) æˆ– None
        """
        result1 = None
        result2 = None
        
        # å°è¯•ä»è¾“å‡ºé˜Ÿåˆ—è·å–ç»“æœ
        try:
            while not self.output_queue1.empty():
                fid, boxes, scores, classes = self.output_queue1.get_nowait()
                with self.cache_lock:
                    self.results_cache[('m1', fid)] = (boxes, scores, classes)
        except:
            pass
        
        try:
            while not self.output_queue2.empty():
                fid, boxes, scores, classes = self.output_queue2.get_nowait()
                with self.cache_lock:
                    self.results_cache[('m2', fid)] = (boxes, scores, classes)
        except:
            pass
        
        # ä»ç¼“å­˜è·å–ç»“æœ
        with self.cache_lock:
            if ('m1', frame_id) in self.results_cache:
                result1 = self.results_cache.pop(('m1', frame_id))
            if ('m2', frame_id) in self.results_cache:
                result2 = self.results_cache.pop(('m2', frame_id))
            
            # æ¸…ç†æ—§ç¼“å­˜
            old_keys = [k for k in self.results_cache.keys() if k[1] < frame_id - 10]
            for k in old_keys:
                del self.results_cache[k]
        
        return result1, result2
    
    def draw_results(self, frame, result1, result2):
        """
        ç»˜åˆ¶ä¸¤ä¸ªæ¨¡å‹çš„æ£€æµ‹ç»“æœ
        
        Args:
            frame: è¾“å…¥å¸§
            result1: æ¨¡å‹1ç»“æœ
            result2: æ¨¡å‹2ç»“æœ
        
        Returns:
            frame: ç»˜åˆ¶åçš„å¸§
        """
        if result1 is not None:
            boxes, scores, classes = result1
            frame = self.detector1.draw(frame, boxes, scores, classes, self.label_prefix1)
        
        if result2 is not None:
            boxes, scores, classes = result2
            frame = self.detector2.draw(frame, boxes, scores, classes, self.label_prefix2)
        
        return frame


def main():
    """ä¸»å‡½æ•°ï¼šåŒæ¨¡å‹æ‘„åƒå¤´å®æ—¶æ£€æµ‹"""
    
    print("=" * 70)
    print("ğŸ¥ åŒæ¨¡å‹æ‘„åƒå¤´å®æ—¶æ£€æµ‹ - å¤šçº¿ç¨‹ç‰ˆæœ¬")
    print("=" * 70)
    
    # ========== é…ç½®ä¸¤ä¸ªæ¨¡å‹ ==========
    # æ¨¡å‹1é…ç½®
    model1_config = {
        'model_path': '/home/sunrise/RDK_infer/yolov11/yolov11_model1.bin',
        'conf_thresh': 0.3,
        'nms_thresh': 0.5,
        'cls_num': 80,
        'class_names': None,  # ä½¿ç”¨é»˜è®¤COCOç±»åˆ«
        'name': 'YOLO-Model1',
        'label_prefix': '[M1]',
    }
    
    # æ¨¡å‹2é…ç½®
    model2_config = {
        'model_path': '/home/sunrise/RDK_infer/yolov11/yolov11_model2.bin',
        'conf_thresh': 0.3,
        'nms_thresh': 0.5,
        'cls_num': 80,
        'class_names': None,  # ä½¿ç”¨é»˜è®¤COCOç±»åˆ«
        'name': 'YOLO-Model2',
        'label_prefix': '[M2]',
    }
    
    # åˆå§‹åŒ–åŒæ¨¡å‹æ¨ç†ç³»ç»Ÿ
    dual_infer = DualModelInference(model1_config, model2_config)
    
    # æ‰“å¼€æ‘„åƒå¤´
    # USBæ‘„åƒå¤´ä½¿ç”¨0ï¼ŒMIPIæ‘„åƒå¤´ä½¿ç”¨8
    cap = cv2.VideoCapture(0)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    
    if not cap.isOpened():
        print("âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
        return
    
    print("\nğŸ“¹ æ‘„åƒå¤´å·²æ‰“å¼€ (640x480)")
    print("ğŸ¬ å¼€å§‹åŒæ¨¡å‹å®æ—¶æ£€æµ‹ (æŒ‰ 'q' é€€å‡º)")
    print("-" * 70)
    
    # è®¾ç½®æ˜¾ç¤ºæƒé™ï¼ˆé€šè¿‡SSHè¿è¡Œæ—¶éœ€è¦ï¼‰
    import os
    os.environ['DISPLAY'] = ':0'
    
    # å¯åŠ¨æ¨ç†çº¿ç¨‹
    dual_infer.start()
    
    # FPSç»Ÿè®¡
    fps_list = []
    frame_count = 0
    
    # ä¸Šä¸€å¸§çš„ç»“æœï¼ˆç”¨äºå¹³æ»‘æ˜¾ç¤ºï¼‰
    last_result1 = None
    last_result2 = None
    
    try:
        while True:
            # è¯»å–å¸§
            ret, frame = cap.read()
            if not ret:
                print("âš ï¸  æ— æ³•è¯»å–æ‘„åƒå¤´å¸§")
                break
            
            # è®¡æ—¶å¼€å§‹
            start = time.time()
            
            # æäº¤å¸§è¿›è¡Œæ¨ç†
            dual_infer.submit_frame(frame, frame_count)
            
            # è·å–æ¨ç†ç»“æœ
            result1, result2 = dual_infer.get_results(frame_count)
            
            # ä½¿ç”¨æœ€æ–°ç»“æœæˆ–ä¸Šä¸€å¸§ç»“æœ
            if result1 is not None:
                last_result1 = result1
            if result2 is not None:
                last_result2 = result2
            
            # ç»˜åˆ¶ç»“æœ
            result_frame = frame.copy()
            result_frame = dual_infer.draw_results(result_frame, last_result1, last_result2)
            
            # è®¡ç®—FPS
            elapsed = time.time() - start
            fps = 1.0 / max(elapsed, 0.001)
            fps_list.append(fps)
            if len(fps_list) > 30:
                fps_list.pop(0)
            avg_fps = np.mean(fps_list)
            
            # ç»Ÿè®¡æ£€æµ‹æ•°é‡
            count1 = len(last_result1[0]) if last_result1 is not None else 0
            count2 = len(last_result2[0]) if last_result2 is not None else 0
            
            # åœ¨å›¾ç‰‡ä¸Šæ˜¾ç¤ºFPSå’Œæ£€æµ‹æ•°é‡
            cv2.putText(result_frame, f"FPS: {avg_fps:.1f}", (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            cv2.putText(result_frame, f"Model1: {count1} objs", (10, 70),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 200, 255), 2)
            cv2.putText(result_frame, f"Model2: {count2} objs", (10, 100),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 200, 0), 2)
            
            # æ˜¾ç¤ºç”»é¢ï¼ˆä¼šæ˜¾ç¤ºåœ¨HDMIæ˜¾ç¤ºå™¨ä¸Šï¼‰
            cv2.imshow('Dual Model Detection', result_frame)
            
            # ç»ˆç«¯æ—¥å¿—
            frame_count += 1
            if frame_count % 30 == 0:
                print(f"å¸§: {frame_count:4d} | FPS: {avg_fps:5.1f} | "
                      f"M1æ£€æµ‹: {count1:2d} | M2æ£€æµ‹: {count2:2d}")
            
            # æŒ‰'q'é€€å‡º
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
    
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­ (Ctrl+C)")
    
    finally:
        # åœæ­¢æ¨ç†çº¿ç¨‹
        dual_infer.stop()
        
        # æ¸…ç†èµ„æº
        cap.release()
        cv2.destroyAllWindows()
        
        if len(fps_list) > 0:
            print("\n" + "=" * 70)
            print("ğŸ“Š æœ€ç»ˆç»Ÿè®¡")
            print("=" * 70)
            print(f"æ€»å¸§æ•°: {frame_count}")
            print(f"å¹³å‡FPS: {np.mean(fps_list):.1f}")
            print("=" * 70)
        
        print("\nâœ… ç¨‹åºç»“æŸ")


def main_sync():
    """
    åŒæ­¥ç‰ˆæœ¬ä¸»å‡½æ•°ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰
    ä¸ä½¿ç”¨å¤šçº¿ç¨‹ï¼Œé¡ºåºæ‰§è¡Œä¸¤ä¸ªæ¨¡å‹çš„æ¨ç†
    é€‚ç”¨äºè°ƒè¯•æˆ–èµ„æºå—é™çš„æƒ…å†µ
    """
    
    print("=" * 70)
    print("ğŸ¥ åŒæ¨¡å‹æ‘„åƒå¤´å®æ—¶æ£€æµ‹ - åŒæ­¥ç‰ˆæœ¬")
    print("=" * 70)
    
    # ========== é…ç½®ä¸¤ä¸ªæ¨¡å‹ ==========
    # æ¨¡å‹1é…ç½®
    detector1 = YOLOv11Detector(
        model_path='/home/sunrise/RDK_infer/yolov11/yolov11_model1.bin',
        conf_thresh=0.3,
        nms_thresh=0.5,
        cls_num=80,
        color_offset=0,
        name='YOLO-Model1'
    )
    
    # æ¨¡å‹2é…ç½®
    detector2 = YOLOv11Detector(
        model_path='/home/sunrise/RDK_infer/yolov11/yolov11_model2.bin',
        conf_thresh=0.3,
        nms_thresh=0.5,
        cls_num=80,
        color_offset=100,
        name='YOLO-Model2'
    )
    
    # æ‰“å¼€æ‘„åƒå¤´
    cap = cv2.VideoCapture(0)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    
    if not cap.isOpened():
        print("âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
        return
    
    print("\nğŸ“¹ æ‘„åƒå¤´å·²æ‰“å¼€ (640x480)")
    print("ğŸ¬ å¼€å§‹åŒæ¨¡å‹å®æ—¶æ£€æµ‹ (æŒ‰ 'q' é€€å‡º)")
    print("-" * 70)
    
    # è®¾ç½®æ˜¾ç¤ºæƒé™
    import os
    os.environ['DISPLAY'] = ':0'
    
    # FPSç»Ÿè®¡
    fps_list = []
    frame_count = 0
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                print("âš ï¸  æ— æ³•è¯»å–æ‘„åƒå¤´å¸§")
                break
            
            start = time.time()
            
            # å…±äº«é¢„å¤„ç†
            nv12, scale, pad_left, pad_top = detector1.bgr_to_nv12(frame)
            preprocess_info = (scale, pad_left, pad_top)
            
            # é¡ºåºæ‰§è¡Œä¸¤ä¸ªæ¨¡å‹æ¨ç†
            boxes1, scores1, classes1 = detector1.detect(frame, nv12, preprocess_info)
            boxes2, scores2, classes2 = detector2.detect(frame, nv12, preprocess_info)
            
            # ç»˜åˆ¶ç»“æœ
            result_frame = frame.copy()
            result_frame = detector1.draw(result_frame, boxes1, scores1, classes1, "[M1]")
            result_frame = detector2.draw(result_frame, boxes2, scores2, classes2, "[M2]")
            
            # è®¡ç®—FPS
            elapsed = time.time() - start
            fps = 1.0 / max(elapsed, 0.001)
            fps_list.append(fps)
            if len(fps_list) > 30:
                fps_list.pop(0)
            avg_fps = np.mean(fps_list)
            
            # æ˜¾ç¤ºä¿¡æ¯
            cv2.putText(result_frame, f"FPS: {avg_fps:.1f}", (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            cv2.putText(result_frame, f"Model1: {len(boxes1)} objs", (10, 70),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 200, 255), 2)
            cv2.putText(result_frame, f"Model2: {len(boxes2)} objs", (10, 100),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 200, 0), 2)
            
            cv2.imshow('Dual Model Detection (Sync)', result_frame)
            
            frame_count += 1
            if frame_count % 30 == 0:
                print(f"å¸§: {frame_count:4d} | FPS: {avg_fps:5.1f} | "
                      f"M1æ£€æµ‹: {len(boxes1):2d} | M2æ£€æµ‹: {len(boxes2):2d}")
            
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
    
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­ (Ctrl+C)")
    
    finally:
        cap.release()
        cv2.destroyAllWindows()
        
        if len(fps_list) > 0:
            print("\n" + "=" * 70)
            print("ğŸ“Š æœ€ç»ˆç»Ÿè®¡")
            print("=" * 70)
            print(f"æ€»å¸§æ•°: {frame_count}")
            print(f"å¹³å‡FPS: {np.mean(fps_list):.1f}")
            print("=" * 70)
        
        print("\nâœ… ç¨‹åºç»“æŸ")


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='åŒæ¨¡å‹æ‘„åƒå¤´å®æ—¶æ£€æµ‹')
    parser.add_argument('--sync', action='store_true',
                        help='ä½¿ç”¨åŒæ­¥æ¨¡å¼ï¼ˆä¸ä½¿ç”¨å¤šçº¿ç¨‹ï¼‰')
    parser.add_argument('--model1', type=str,
                        default='/home/sunrise/RDK_infer/yolov11/yolov11_model1.bin',
                        help='æ¨¡å‹1è·¯å¾„')
    parser.add_argument('--model2', type=str,
                        default='/home/sunrise/RDK_infer/yolov11/yolov11_model2.bin',
                        help='æ¨¡å‹2è·¯å¾„')
    parser.add_argument('--conf1', type=float, default=0.3,
                        help='æ¨¡å‹1ç½®ä¿¡åº¦é˜ˆå€¼')
    parser.add_argument('--conf2', type=float, default=0.3,
                        help='æ¨¡å‹2ç½®ä¿¡åº¦é˜ˆå€¼')
    parser.add_argument('--cls1', type=int, default=80,
                        help='æ¨¡å‹1ç±»åˆ«æ•°é‡')
    parser.add_argument('--cls2', type=int, default=80,
                        help='æ¨¡å‹2ç±»åˆ«æ•°é‡')
    parser.add_argument('--camera', type=int, default=0,
                        help='æ‘„åƒå¤´ID (USB=0, MIPI=8)')
    
    args = parser.parse_args()
    
    if args.sync:
        # åŒæ­¥æ¨¡å¼
        main_sync()
    else:
        # å¤šçº¿ç¨‹æ¨¡å¼ï¼ˆéœ€è¦ä¿®æ”¹mainå‡½æ•°ä»¥æ”¯æŒå‘½ä»¤è¡Œå‚æ•°ï¼‰
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥è°ƒç”¨main()
        # å¦‚éœ€ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå¯ä»¥ä¿®æ”¹main()å‡½æ•°æ¥æ”¶å‚æ•°
        main()